{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3704e353-1ea3-4ef5-a48c-de044638b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "%matplotlib qt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen, surrogate, utils\n",
    "import snntorch.functional as SF\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d66d1-1548-40a8-a078-0e6a3317e5f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236a99fb-f9d8-4f3d-a70c-81235edd301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='/tmp/data/mnist'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bd91a1-569c-42d6-8320-050c62d7e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a0fa6f-da03-4e7d-86e5-d5d5bd801515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_data, model, num_steps):\n",
    "    spk_rec = []\n",
    "    mem_rec = []\n",
    "    utils.reset(model)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        spk, mem = model(input_data)\n",
    "        spk_rec.append(spk)\n",
    "        mem_rec.append(mem)\n",
    "\n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "def batch_acc(data_loader, model, num_steps):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        total = 0\n",
    "        for inputs, targets in iter(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "        \n",
    "            spk_rec, mem_rec = forward_pass(inputs, model, num_steps)\n",
    "            acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "    return acc / total    \n",
    "    \n",
    "class MyEEGSNNModel(nn.Module):\n",
    "    def __init__(self, n_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 12, 5)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(12, 64, 5)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(64*4*4, n_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1, 5))\n",
    "        # self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=(1, 5))\n",
    "        # self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # self.fc1 = nn.Linear(64*14*157, n_outputs)\n",
    "        # self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        cur1 = F.max_pool2d(self.conv1(x), kernel_size=(2, 2))\n",
    "        # cur1 = F.max_pool2d(self.conv1(x), kernel_size=(1, 2))\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "         \n",
    "        cur2 = F.max_pool2d(self.conv2(spk1), kernel_size=(2, 2))\n",
    "        # cur2 = F.max_pool2d(self.conv2(spk1), kernel_size=(1, 2))\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        \n",
    "        cur3 = self.fc1(spk2.view(batch_size, -1))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        return spk3, mem3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d361f245-9ae1-41be-99cc-3b65466e273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "model = MyEEGSNNModel(num_classes).to(\"cuda\")\n",
    "\n",
    "for inputs, targets in tqdm(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    spk_rec, _= forward_pass(inputs, model, num_steps)\n",
    "    print(spk_rec.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f09c3b0-8e3b-46cd-be24-2b9a30894471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:14<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3026, Accuracy: 0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 2.0290, Accuracy: 0.4005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 1.5172, Accuracy: 0.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 1.5029, Accuracy: 0.9171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 1.4988, Accuracy: 0.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "num_classes = 10\n",
    "# num_classes = 4\n",
    "model = MyEEGSNNModel(num_classes).to(\"cuda\")\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "criterion = SF.ce_rate_loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    counter = 0\n",
    "    acc_co = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(iter(train_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        spk_rec, _ = forward_pass(inputs, model, num_steps)\n",
    "        loss = criterion(spk_rec, targets)\n",
    "            \n",
    "        loss.backward()         \n",
    "        optimizer.step()  \n",
    "        epoch_loss += loss.item()\n",
    "        counter+=1\n",
    "\n",
    "        if counter % 75 == 0:\n",
    "        # if counter % 6 == 0:\n",
    "            train_acc += batch_acc(test_loader, model, num_steps)\n",
    "            acc_co+=1\n",
    "        \n",
    "    train_loss = epoch_loss / counter\n",
    "    train_acc = train_acc.item() / acc_co\n",
    "    # train_acc = train_acc.item() / acc_co\n",
    "    loss_history.append(train_loss)\n",
    "    acc_history.append(train_acc)\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee1120-4a6c-4fa1-a4a5-1eb2d808f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for inputs, targets in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        spk_rec, mem_rec = model(inputs, num_steps)\n",
    "        spk_count = torch.sum(spk_rec, dim=0)\n",
    "        outputs = F.softmax(spk_count)\n",
    "        \n",
    "    loss = criterion(outputs, targets)\n",
    "    epoch_loss += loss.item()\n",
    "print(f'Loss: {epoch_loss / len(test_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4fb93-30dd-4b6b-a6f0-7075846c2d00",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4ce8c9-8600-406f-8116-0a0424391d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112/112 [00:02<00:00, 39.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4480, 14, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4480/4480 [00:01<00:00, 3533.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1674dad7100>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"Extras/GAMEEMO_EPOCH/5sec\"\n",
    "epochs_data = []\n",
    "\n",
    "for file in tqdm(os.listdir(folder_path)):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    epochs = mne.read_epochs(file_path, preload=True)\n",
    "    epochs_choice_idx = np.random.choice(epochs.get_data().shape[0], size=40, replace=False)\n",
    "    epochs_choice = epochs.get_data()[epochs_choice_idx]\n",
    "    epochs_data.append(epochs_choice)\n",
    "\n",
    "epochs_data = np.stack(epochs_data, axis=0)\n",
    "epochs_data_reshaped = epochs_data.reshape(-1, epochs_data.shape[-2], epochs_data.shape[-1])\n",
    "print(epochs_data_reshaped.shape)\n",
    "\n",
    "for i in tqdm(range(epochs_data_reshaped.shape[0])):\n",
    "    for j in range(epochs_data_reshaped.shape[1]):\n",
    "        data = epochs_data_reshaped[i,j,:]\n",
    "        epochs_data_reshaped[i,j,:] = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(epochs_data_reshaped[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6528dfa-9d74-4e49-9d56-191fc965ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4480,)\n",
      "(4480, 4)\n",
      "[0 0 0 ... 3 3 3]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "label_list = [0, 1, 2, 3]\n",
    "labels = []\n",
    "for i in range(int(112/4)):\n",
    "    labels.extend(label_list)\n",
    "\n",
    "labels = np.array(labels)\n",
    "labels = np.repeat(labels, int(epochs_data_reshaped.shape[0]/112))\n",
    "oh_labels = np.eye(4)[labels]\n",
    "print(labels.shape)\n",
    "print(oh_labels.shape)\n",
    "np.set_printoptions(threshold=20)\n",
    "print(labels)\n",
    "print(oh_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d425a921-d91f-495c-9f76-bc9b9d2ad9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3360, 14, 640]) torch.Size([1120, 14, 640]) torch.Size([3360]) torch.Size([1120])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(epochs_data_reshaped, labels, test_size=0.25, random_state=56)\n",
    "\n",
    "num_steps = 32\n",
    "device = torch.device(\"cuda\") \n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "batch_size = 80\n",
    "train_dataset = TensorDataset(X_train.unsqueeze(1), y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test.unsqueeze(1), y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b0b3ac-2c22-45ca-af4a-afda5ebfd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7a45bc-c96f-4c91-92b2-135e99f4447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_data, model, num_steps):\n",
    "    spk_rec = []\n",
    "    mem_rec = []\n",
    "    utils.reset(model)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        spk, mem = model(input_data)\n",
    "        spk_rec.append(spk)\n",
    "        mem_rec.append(mem)\n",
    "\n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "def batch_acc(data_loader, model, num_steps):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        total = 0\n",
    "        for inputs, targets in iter(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "        \n",
    "            spk_rec, mem_rec = forward_pass(inputs, model, num_steps)\n",
    "            acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "    return acc / total    \n",
    "    \n",
    "class MyEEGSNNModel(nn.Module):\n",
    "    def __init__(self, n_outputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(1, 12, 5)\n",
    "        # self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # self.conv2 = nn.Conv2d(12, 64, 5)\n",
    "        # self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        # self.fc1 = nn.Linear(64*4*4, n_outputs)\n",
    "        # self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1, 5))\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=(1, 5))\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(64*14*157, n_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # cur1 = F.max_pool2d(self.conv1(x), kernel_size=(2, 2))\n",
    "        cur1 = F.max_pool2d(self.conv1(x), kernel_size=(1, 2))\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "         \n",
    "        # cur2 = F.max_pool2d(self.conv2(spk1), kernel_size=(2, 2))\n",
    "        cur2 = F.max_pool2d(self.conv2(spk1), kernel_size=(1, 2))\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        \n",
    "        cur3 = self.fc1(spk2.view(batch_size, -1))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        return spk3, mem3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96310131-81b2-44ba-a329-153ab095c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/42 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 80, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "model = MyEEGSNNModel(num_classes).to(\"cuda\")\n",
    "\n",
    "for inputs, targets in tqdm(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    spk_rec, _= forward_pass(inputs, model, num_steps)\n",
    "    print(spk_rec.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b1af9b-2015-4c74-80f6-a0cd6c5c6fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [02:20<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3863, Accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [02:19<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 1.3863, Accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [02:18<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 1.3863, Accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [02:18<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 1.3863, Accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [02:19<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 1.3863, Accuracy: 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "# num_classes = 10\n",
    "num_classes = 4\n",
    "model = MyEEGSNNModel(num_classes).to(\"cuda\")\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "criterion = SF.ce_rate_loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    counter = 0\n",
    "    acc_co = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(iter(train_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        spk_rec, _ = forward_pass(inputs, model, num_steps)\n",
    "        loss = criterion(spk_rec, targets)\n",
    "            \n",
    "        loss.backward()         \n",
    "        optimizer.step()  \n",
    "        epoch_loss += loss.item()\n",
    "        counter+=1\n",
    "\n",
    "        # if counter % 75 == 0:\n",
    "        if counter % 6 == 0:\n",
    "            train_acc += batch_acc(test_loader, model, num_steps)\n",
    "            acc_co+=1\n",
    "        \n",
    "    train_loss = epoch_loss / counter\n",
    "    train_acc = train_acc.item() / acc_co\n",
    "    loss_history.append(train_loss)\n",
    "    acc_history.append(train_acc)\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ec1ac-a15c-4492-a112-af017b283d81",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360427bf-6faa-4430-95b1-846bb44a9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='/tmp/data/mnist'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = 50\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.conv1 = nn.Conv2d(1, 12, 5)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(12, 64, 5)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(64*4*4, 10)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        cur1 = F.max_pool2d(self.conv1(x), 2)\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "        cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "        cur3 = self.fc1(spk2.view(batch_size, -1))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        return spk3, mem3\n",
    "\n",
    "def forward_pass(net, num_steps, data):\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps):\n",
    "      spk_out, mem_out = net(data)\n",
    "      spk_rec.append(spk_out)\n",
    "      mem_rec.append(mem_out)\n",
    "\n",
    "  return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total\n",
    "\n",
    "# already imported snntorch.functional as SF\n",
    "loss_fn = SF.ce_rate_loss()\n",
    "\n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_acc_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training loop\n",
    "    for data, targets in tqdm(iter(train_loader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        if counter % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "    \n",
    "                # Test set forward pass\n",
    "                test_acc = batch_accuracy(test_loader, net, num_steps)\n",
    "                print(f\"Iteration {counter}, Test Acc: {test_acc * 100:.2f}%\\n\")\n",
    "                test_acc_hist.append(test_acc.item())\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "print(batch_accuracy(test_loader, net, num_steps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch310",
   "language": "python",
   "name": "trch310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
